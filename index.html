<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>AI in Real</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
    <meta name="keywords" content="Concerted Responsive web template, Bootstrap Web Templates, Flat Web Templates, Android Compatible web template,
	Smartphone Compatible web template, free webdesigns for Nokia, Samsung, LG, SonyErricsson, Motorola web design"/>
    <script type="applijewelleryion/x-javascript">
         addEventListener("load", function() { setTimeout(hideURLbar, 0); }, false); function hideURLbar(){ window.scrollTo(0,1); }

    </script>
    <link href="css/bootstrap.css" rel='stylesheet' type='text/css'/>
    <!-- Custom Theme files -->
    <link href='//fonts.googleapis.com/css?family=Viga' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Roboto+Condensed' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:400,700,300' rel='stylesheet' type='text/css'>
    <link href="css/style.css" rel='stylesheet' type='text/css'/>
    <link rel="stylesheet" href="css/flexslider.css" type="text/css" media="screen"/>
    <script src="js/jquery-1.11.1.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script type="text/javascript" src="js/move-top.js"></script>
    <script type="text/javascript" src="js/easing.js"></script>
    <script type="text/javascript">
        jQuery(document).ready(function ($) {
            $(".scroll").click(function (event) {
                event.preventDefault();
                $('html,body').animate({scrollTop: $(this.hash).offset().top}, 1200);
            });
        });
    </script>
    <script type="text/javascript">
        $(document).ready(function () {

            $().UItoTop({easingType: 'easeOutQuart'});
        });
    </script>
    <script src="js/jquery.chocolat.js"></script>
    <link rel="stylesheet" href="css/chocolat.css" type="text/css" media="all"/>
    <!--light-box-files -->
    <script type="text/javascript">
        $(function () {
            $('#example1 a').Chocolat();
        });
    </script>
    <script type="text/javascript">
        $(function () {
            $('#portfolio a').Chocolat();
        });
    </script>
    <!-- animation-effect -->
    <link href="css/animate.min.css" rel="stylesheet">
    <script src="js/wow.min.js"></script>
    <script>
        new WOW().init();
    </script>
    <!-- //animation-effect -->
</head>

<body>
<div class="banner w3l-1">
    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <div class="navbar-header wow fadeInLeft animated animated" data-wow-delay=".5s" style="visibility: visible; animation-delay: 0.5s; animation-name: fadeInLeft;">

                <!--toggle button-->
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
            </div>
            <div id="navbar" class="navbar-collapse collapse">
                <ul class="nav navbar-nav">
                    <li><a href="#cfp" class="scroll">Call for Papers</a></li>
                    <li><a href="#schedule" class="scroll">Schedule</a></li>
                    <li><a href="#speakers" class="scroll">Invited Speakers</a></li>
                    <li><a href="#papers" class="scroll">Accepted Papers</a></li>
                    <li><a href="#organizers" class="scroll">Organizers</a></li>
                </ul>
                <div class="clearfix"></div>
            </div>
        </div>
    </nav>
</div>

<div class="container"  style="padding-top: 100px;">
    <div class="page-header">
        <div class="row">
            <div class="col-xs-12">
                <center>
                    <h1>Future Impact of AI in Real-World Applications</h1>
                </center>
                <center>
                    <h3>ACPR 2021 Workshop, Jeju Island, Korea</h3>
                </center>
                <br>
                <center>
                    Friday November 12 2021, 13:00pm - 17:00pm
                </center>
                <center>
                    Location: <b>ICC Jeju, Samda Hall</b>
                </center>
            </div>

        </div>
    </div>
    <hr>

    <div class="row" id="intro">
        <div class="col-md-12" style="text-align: center">
            <img width="90%" src="images/jeju-intro.jpg">
        </div>
    </div>
    <hr>
    <div class="row" id="cfp">
        <div class="col-xs-12">
            <h2>Introduction</h2>
            <br>
            <div class="row" style="padding: 15px 0px 15px 25px;">
                <div class="col-xs-12">
                    <p>
                        The workshop will focus on various area of future impact of AI. The aim of this workshop is to bring
                        researchers and scientists from academia, medical area with engineers from industry together to discuss
                        about various impact of cutting-edge technologies of AI in future society. The workshop will take a deep
                        dive into the capabilities of Edge Insights for Academia and Industrial via a tutorial utilizing
                        real-world AI applications. For example, breast cancer can be detected via smartphone level infrared
                        camera for detecting lesion and target mass at home. Despite the existence of some commercial AI systems
                        such as autonomous vehicle, we are at the beginning of a long research pathway towards a future
                        generation of deep AI. The workshop focuses on numerical and computational aspects of future impact of
                        AI and on these relations to various AI techniques.
                    </p>
                    <br>
                    <h4>Call for papers</h4>
                    <p>
                        We welcome submissions on the following topics, including but not limited to:
                    </p>
                    <ur>
                        <li>Image Generation and Translation</li>
                        <li>Semantic segmentation/ Instance Segmentation</li>
                        <li>Recognition: detection, tracking, Anomaly detection, localization</li>
                        <li>Image processing: denoising, enhancement, super resolution</li>
                        <li>3D computer vision, stereo matching</li>
                        <li>NLP (natural language processing)</li>
                        <li>Voice Recognition: STT(Speech to Text)</li>
                        <li>Reinforcement Learning</li>
                        <li>Sensor fusion with AI</li>
                        <li>Game with AI</li>

                    </ur>
                </div>
            </div>
        </div>
    </div>


    <hr>
    <div class="row" id="dates">
        <div class="col-xs-12">
            <h2>Important Dates</h2>
            <br>
            <table class="table table-striped">
                <tbody>
                <tr>
                    <td>Paper Submission Deadline</td>
                    <!--                    <td><s style="color:darkred;">July 29</s> July 31, 2019 (23:59 Pacific time)</td>-->
                    <td>October 10, 2021 (23:59 Pacific time)</td>
                </tr>
                <tr>
                    <td>Notification to Authors</td>
                    <!--                    <td><s style="color:darkred;">August 16</s> August 18, 2019</td>-->
                    <td>October 17, 2021</td>
                </tr>
                <tr>
                    <td>Camera-Ready Deadline</td>
                    <td>October 24, 2021</td>
                </tr>
                <tr>
                    <td>Workshop Date</td>
                    <td>November 12, 2021 (Afternoon)</td>
                </tr>
                </tbody>
            </table>
        </div>
    </div>

    <hr>
    <div class="row">
        <div class="col-xs-12">
            <h2>Submission</h2>
            <br>
            <div class="row" style="padding: 0px 0px 15px 25px;">
                <div class="col-md-9">
                    <p>
<!--                         Papers will be limited to 2~3 pages according to the LNCS format (c.f. main conference authors guidelines).
                        All papers will be reviewed by at least two reviewers with double blind policy.
                        Papers will be selected based on relevance, significance and novelty of results, technical merit, and clarity of presentation.
                        Papers will be published in workshop homepage. -->
			Extended Abstracts: Participants are encouraged to submit preliminary ideas that have not been previously published in conferences or journals.
			We also invite papers published in other conferences and journals (2021 only) to facilitate new collaborations.
			Submissions may consist of one page abstract and one additional page for references (using the template described above).
			The expanded abstract will be posted on the website only during the workshop period.
                    </p>
                    <p>
                        All the papers should be submitted to workshop chairs, Prof. Lee (segeberg@kmu.ac.kr) and Prof. Ko (niceko@kmu.ac.kr)
                    </p>
                </div>
            </div>


        </div>
    </div>


    <hr>
    <div class="row" id="schedule">
        <div class="col-xs-12">
            <h2>Workshop Schedule</h2>
            <br>
            <table class="table schedule" style="border:none !important;">
                <thead class="thead-light">
                <tr>
                    <th>#</th>
                    <th>Time</th>
                    <th>Item</th>
		    <th> </th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>1</td>
                    <td>13:00pm - 13:05pm</td>
                    <td>Opening Remarks (Prof. Jong-Ha Lee)</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td>13:05pm - 13:40pm</td>
                    <td>Keynote Talk: Prof. Soo Hyung Kim (Chonnam National University)</td>
                </tr>
                <tr>
                    <td>3</td>
                    <td>13:40pm - 14:10pm</td>
			<td>Keynote Talk: Prof. Chang-Hee Won (Temple University)</td>
                </tr>
                <tr>
                    <td></td>
                    <td>14:10pm - 14:30pm</td>
                    <td>Coffee Break</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td>14:30pm - 15:05pm</td>
			<td>Keynote Talk: Prof. Suha Kwak (POSTECH)</td>
                </tr>
		<tr>
                    <td>5</td>
                    <td>15:05pm - 15:35pm</td>
			<td>Keynote Talk: Prof. Chih-Chung Hsu (National Cheng Kung University)</td>
                </tr>
		<tr>
                    <td></td>
                    <td>15:35pm - 15:45pm</td>
                    <td>Coffee Break</td>
                </tr>
                <tr>
                    <td>6</td>
                    <td>15:45pm - 16:45pm</td>
                    <td>Oral Presentation</td>
                </tr>
		<tr>
                    <td></td>
                    <td>15:45pm - 15:55pm</td>
                    <td>
                	G2CN: Geometric Graph Convolutional Network for Facial Expression Recognition
			</td>
                </tr>
		<tr>
                    <td></td>
                    <td>15:55pm - 16:05pm</td>
                    <td>Histological Image Segmentation and Classification Using Entropy-Based Convolutional Module</td>
                </tr>
		<tr>
                    <td></td>
                    <td>16:05pm - 16:15pm</td>
                    <td>Vision Trnasformer Based Dynamic Facial Emotion Recognition</td>
                </tr>
		<tr>
                    <td></td>
                    <td>16:15pm - 16:25pm</td>
                    <td>Development of a system capable of diagnosing and treating Alzheimer's disease: a technique experiment using cadaver</td>
                </tr>
		<tr>
                    <td></td>
                    <td>16:25pm - 16:35pm</td>
                    <td>Remote Bio Vision: Perfusion Imaging Based Non-Contact Biosignal Measurement Method</td>
                </tr>
		<tr>
                    <td></td>
                    <td>16:35pm - 16:45pm</td>
                    <td>Calibrating a Multiple-View Thermal Camera</td>
                </tr>
		<tr>
                    <td>7</td>
                    <td>16:45pm - 16:55pm</td>
                    <td>Closing Remarks (Prof. Byoung Chul Ko)</td>
                </tr>
                </tbody>
            </table>
        </div>
    </div>

    <hr>
    <div class="row" id="speakers">
        <div class="col-xs-12">
            <h2>Invited Keynote Speakers</h2>
            <div class="row speaker" id="shkim">
                <div class="col-sm-3 speaker-pic">
                    <a href="http://pr.jnu.ac.kr/shkim/">
                        <img class="people-pic" src="images/shkim.jpg">
                    </a>
                    <div class="people-name">
                        <a href="http://pr.jnu.ac.kr/shkim/">Prof. SooHyung Kim</a>
                        <h6>Chonnam National University</h6>
                    </div>
                </div>
                <div class="col-md-9">
                    <h4>Survival Time Prediction for Cancer Patients based on Multi-Modal Medical Data</h4>
                    <p class="speaker-abstract">
                        A deep learning approach for survival time prediction is introduced, which utilizes clinomics, radiomics, and pathomics for a cancer patient. Clinical examples for lung cancer cases show that the multi-modal approach is promising for precision medicine.
                    </p>
                </div>
            </div>
		<br>
		<div class="row speaker" id="chwon">
                <div class="col-sm-3 speaker-pic">
                    <a href="https://www.temple.edu/about/faculty-staff/chang-hee-won-cwon">
                        <img class="people-pic" src="https://www.temple.edu/sites/www/files/tu-people/cwon.jpg">
                    </a>
                    <div class="people-name">
                        <a href="https://www.temple.edu/about/faculty-staff/chang-hee-won-cwon">Prof. Chang-Hee Won</a>
                        <h6>Temple University</h6>
                    </div>
                </div>
                <div class="col-md-9">
                    <h4>Bimodal Imaging of Breast Cancer Using Profile Diagrams and Convolution Neural Network</h4>
                    <p class="speaker-abstract">
                        In this talk, we will discuss a bimodal imaging system, which consists of tactile and spectral sensors. A Tactile Profile Diagram is a pictorial representation of the mechanical properties of the imaged tumor. A Multispectral Profile Diagram is a representative pattern image of the breast tissue’s spectral properties. To classify the profile diagrams, we employ the Convolutional Neural Network method. The human experimental results demonstrate the ability of the developed method to classify and quantify breast cancer. Finally, we describe a method to calculate Multimodal Index for the malignancy risk assessment using profile diagrams and health records.
                    </p>
                </div>
            </div>
                 <br>
		<div class="row speaker" id="shk">
                <div class="col-sm-3 speaker-pic">
                    <a href="http://cvlab.postech.ac.kr/~suhakwak/">
                        <img class="people-pic" src="images/shk.png">
                    </a>
                    <div class="people-name">
                        <a href="http://cvlab.postech.ac.kr/~suhakwak/">Prof. Suha Kwak</a>
                        <h6>POSTECH</h6>
                    </div>
                </div>
                <div class="col-md-9">
                    <h4>Loss functions for Deep metric learning</h4>
                    <p class="speaker-abstract">
                        TBA
                    </p>
                </div>
            </div>
		<br>
		<div class="row speaker" id="cch">
                <div class="col-sm-3 speaker-pic">
                    <a href="https://cchsu.info/wordpress/">
                        <img class="people-pic" src="https://scholar.googleusercontent.com/citations?view_op=view_photo&user=mIWRYc4AAAAJ&citpid=7">
                    </a>
                    <div class="people-name">
                        <a href="https://cchsu.info/wordpress/">Prof. Chih-Chung Hsu</a>
                        <h6>National Cheng Kung University</h6>
                    </div>
                </div>
                <div class="col-md-9">
                    <h4>Multilinear Data Super-Resolution: 2D to ND</h4>
                    
                    <p class="speaker-abstract">
                        With the rapid growth of deep learning applications, conventional image restoration tasks such as super-resolution has made significant progress in recent years. Specifically, the convolutional neural network (CNN)-based super-resolution has achieved excellent performance, while many super-resolution networks were proposed to improve the fidelity and visual quality.
In this talk, I would like to introduce the multi-perspective image super-resolution: from the structured facial (2-D) to hyperspectral (172-D) image super-resolution by exploring the image prior, the correlation between spectrum, and special tricks in CNN for super-resolution tasks. I will also make some open issues for further research on super-resolution tasks in this talk.  
                    </p>
                </div>
            </div>
	<br>
		
<!-- 		<div class="row speaker" id="jhl">
                <div class="col-sm-3 speaker-pic">
                    <a href="http://203.247.8.200/index.php/Faculty:Jong-Ha_Lee_(KR)">
                        <img class="people-pic" src="images/250px-Lee5.jpg">
                    </a>
                    <div class="people-name">
                        <a href="http://203.247.8.200/index.php/Faculty:Jong-Ha_Lee_(KR)">Prof. Jong-Ha Lee</a>
                        <h6>Keimyung University</h6>
                    </div>
                </div>
                <div class="col-md-9">
                    <h4>TBA</h4>
                    <br>
                    <b>Abstract</b>
                    <p class="speaker-abstract">
                        TBA
                    </p>
                </div>
            </div> -->
		
        </div>
    </div>


    <hr>
    <div class="row" id="papers">
        <div class="col-xs-12">
            <h2>Accepted Papers</h2>

            <div class="paper">
                <span class="title">G2CN: Geometric Graph Convolutional Network for Facial Expression Recognition</span>
                <span class="authors">Hyung jin Kim, Byoung Chul Ko</span>
                <div class="btn-group btn-group-xs" role="group">
                    <!--<button class="btn btn-poster-id">Poster #118</button>-->
                    <a class="btn btn-default" target="_blank"
                       href=""><i
                            class="fas fa-file-pdf" aria-hidden="true"></i> PDF </a>
                    <!--<a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>-->
                </div>
            </div>
		
		 <div class="paper">
                <span class="title">Vision Trnasformer Based Dynamic Facial Emotion Recognition</span>
                <span class="authors">Dasom Ahn, Sangwon Kim, Byoung Chul Ko</span>
                <div class="btn-group btn-group-xs" role="group">
                    <!--<button class="btn btn-poster-id">Poster #118</button>-->
                    <a class="btn btn-default" target="_blank"
                       href=""><i
                            class="fas fa-file-pdf" aria-hidden="true"></i> PDF </a>
                    <!--<a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>-->
                </div>
            </div>
		
		 <div class="paper">
                <span class="title">Histological Image Segmentation and Classification Using Entropy-Based Convolutional Module</span>
                <span class="authors">Hwa-Rang Kim, Kwang-Ju Kim, Kil-Taek Lim, Doo-Hyun Choi</span>
                <div class="btn-group btn-group-xs" role="group">
                    <!--<button class="btn btn-poster-id">Poster #118</button>-->
                    <a class="btn btn-default" target="_blank"
                       href=""><i
                            class="fas fa-file-pdf" aria-hidden="true"></i> PDF </a>
                    <!--<a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>-->
                </div>
            </div>
		
		
		<div class="paper">
                <span class="title">Development of a system capable of diagnosing and treating Alzheimer's disease: a technique experiment using cadaver</span>
                <span class="authors">Eun Bin Park, Jong-Ha Lee</span>
                <div class="btn-group btn-group-xs" role="group">
                    <!--<button class="btn btn-poster-id">Poster #118</button>-->
                    <a class="btn btn-default" target="_blank"
                       href=""><i
                            class="fas fa-file-pdf" aria-hidden="true"></i> PDF </a>
                    <!--<a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>-->
                </div>
            </div>
		
	<div class="paper">
                <span class="title">Remote Bio Vision: Perfusion Imaging Based Non-Contact Biosignal Measurement Method</span>
                <span class="authors">Chan Il Kim, Jong-Ha Lee</span>
                <div class="btn-group btn-group-xs" role="group">
                    <!--<button class="btn btn-poster-id">Poster #118</button>-->
                    <a class="btn btn-default" target="_blank"
                       href=""><i
                            class="fas fa-file-pdf" aria-hidden="true"></i> PDF </a>
                    <!--<a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>-->
                </div>
            </div>
		
	<div class="paper">
                <span class="title">Calibrating a Multiple-View Thermal Camera</span>
                <span class="authors">Ju O Kim, Ji Eun Kim, Deokwoo Lee</span>
                <div class="btn-group btn-group-xs" role="group">
                    <!--<button class="btn btn-poster-id">Poster #118</button>-->
                    <a class="btn btn-default" target="_blank"
                       href=""><i
                            class="fas fa-file-pdf" aria-hidden="true"></i> PDF </a>
                    <!--<a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>-->
                </div>
            </div>

<!--            <div class="paper">-->
<!--                <span class="title">A Generalized and Robust Method Towards Practical Gaze Estimation on Smart Phone</span>-->
<!--                <span class="authors">Tianchu Guo, Yongchao Liu, Hui Zhang, Xiabing Liu, Youngjun Kwak, ByungIn Yoo, Jae-Joon Han, Changkyu Choi</span>-->
<!--                <div class="btn-group btn-group-xs" role="group">-->
<!--                    &lt;!&ndash;<button class="btn btn-poster-id">Poster #118</button>&ndash;&gt;-->
<!--                    <a class="btn btn-default" target="_blank"-->
<!--                       href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/GAZE/Guo_A_Generalized_and_Robust_Method_Towards_Practical_Gaze_Estimation_on_ICCVW_2019_paper.pdf"><i-->
<!--                            class="fas fa-file-pdf" aria-hidden="true"></i> PDF </a>-->
<!--                    &lt;!&ndash;<a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>&ndash;&gt;-->
<!--                </div>-->
<!--            </div>-->

<!--            <div class="paper">-->
<!--                <span class="title">A Generalized and Robust Method Towards Practical Gaze Estimation on Smart Phone</span>-->
<!--                <span class="authors">Tianchu Guo, Yongchao Liu, Hui Zhang, Xiabing Liu, Youngjun Kwak, ByungIn Yoo, Jae-Joon Han, Changkyu Choi</span>-->
<!--                <div class="btn-group btn-group-xs" role="group">-->
<!--                    &lt;!&ndash;<button class="btn btn-poster-id">Poster #118</button>&ndash;&gt;-->
<!--                    <a class="btn btn-default" target="_blank"-->
<!--                       href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/GAZE/Guo_A_Generalized_and_Robust_Method_Towards_Practical_Gaze_Estimation_on_ICCVW_2019_paper.pdf"><i-->
<!--                            class="fas fa-file-pdf" aria-hidden="true"></i> PDF </a>-->
<!--                    &lt;!&ndash;<a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>&ndash;&gt;-->
<!--                </div>-->
<!--            </div>-->
        </div>
    </div>

<!--    <hr>-->
<!--    <div class="row">-->
<!--        <div class="col-xs-12">-->
<!--            <h2>Accepted Posters</h2>-->
<!--            <div class="paper">-->
<!--                <span class="title">A Generalized and Robust Method Towards Practical Gaze Estimation on Smart Phone</span>-->
<!--                <span class="authors">Tianchu Guo, Yongchao Liu, Hui Zhang, Xiabing Liu, Youngjun Kwak, ByungIn Yoo, Jae-Joon Han, Changkyu Choi</span>-->
<!--                <div class="btn-group btn-group-xs" role="group">-->
<!--                    &lt;!&ndash;<button class="btn btn-poster-id">Poster #118</button>&ndash;&gt;-->
<!--                    <a class="btn btn-default" target="_blank"-->
<!--                       href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/GAZE/Guo_A_Generalized_and_Robust_Method_Towards_Practical_Gaze_Estimation_on_ICCVW_2019_paper.pdf"><i-->
<!--                            class="fas fa-file-pdf" aria-hidden="true"></i> PDF </a>-->
<!--                    &lt;!&ndash;<a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>&ndash;&gt;-->
<!--                </div>-->
<!--            </div>-->
<!--            <div class="paper">-->
<!--                <span class="title">A Generalized and Robust Method Towards Practical Gaze Estimation on Smart Phone</span>-->
<!--                <span class="authors">Tianchu Guo, Yongchao Liu, Hui Zhang, Xiabing Liu, Youngjun Kwak, ByungIn Yoo, Jae-Joon Han, Changkyu Choi</span>-->
<!--                <div class="btn-group btn-group-xs" role="group">-->
<!--                    &lt;!&ndash;<button class="btn btn-poster-id">Poster #118</button>&ndash;&gt;-->
<!--                    <a class="btn btn-default" target="_blank"-->
<!--                       href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/GAZE/Guo_A_Generalized_and_Robust_Method_Towards_Practical_Gaze_Estimation_on_ICCVW_2019_paper.pdf"><i-->
<!--                            class="fas fa-file-pdf" aria-hidden="true"></i> PDF </a>-->
<!--                    &lt;!&ndash;<a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>&ndash;&gt;-->
<!--                </div>-->
<!--            </div>-->
<!--        </div>-->
<!--    </div>-->

    <hr>
    <div class="row" id="organizers">
        <div class="col-xs-12">
            <h2>Organizers</h2>
        </div>
    </div>
    <br>
    <div class="row">
<!--        <div class="col-xs-1"></div>-->
        <div class="col-xs-2">
            <a href="http://203.247.8.200/index.php/Faculty:Jong-Ha_Lee_(KR)">
                <img class="people-pic" src="images/250px-Lee5.jpg">
            </a>
            <div class="people-name">
                <a href="http://203.247.8.200/index.php/Faculty:Jong-Ha_Lee_(KR)">Jong-Ha Lee</a>
                <h6>Keimyung University</h6>
            </div>
        </div>
        <div class="col-xs-2">
            <a href="https://cvpr.kmu.ac.kr/member.htm">
                <img class="people-pic" src="images/bcko.jpg">
            </a>
            <div class="people-name">
                <a href="https://cvpr.kmu.ac.kr/member.htm">Byoung Chul Ko</a>
                <h6>Keimyung University</h6>
            </div>
        </div>

    </div>

    <hr>
    <div class="row">
        <div class="col-xs-12">
            <h2>Program Committee</h2>
            <br>
            <div class="row">
                <div class="col-xs-3">
                    <div class="people-name"><a target="_blank" href="http://nisl.kmu.ac.kr/">Yo Han Park</a><h6>
                        Keimyung University</h6></div>
                    <div class="people-name"><a target="_blank"
                                                href="https://sites.google.com/view/dwoolee/deokwoo-lee?authuser=0">Deokwoo
                        Lee</a><h6>Keimyung University</h6></div>
                    <div class="people-name"><a target="_blank" href="https://wwwfr.uni.lu/snt/people/djamila_aouada">Djamila
                        Aouada</a><h6>Université du Luxembourg</h6></div>
                    <div class="people-name"><a target="_blank" href="https://hyungjinchang.wordpress.com/">Hyung Jin
                        Chang</a><h6>University of Birmingham</h6></div>
                    <div class="people-name"><a target="_blank" href="Chang-Hee Won">Chang-Hee Won</a><h6>Temple
                        University</h6></div>
                </div>
                <div class="col-xs-3">
                    <div class="people-name"><a target="_blank"
                                                href="https://engineering.nyu.edu/faculty/shivendra-panwar">Shivendra
                        Panwar</a><h6>New York University</h6></div>
                    <div class="people-name"><a target="_blank" href="https://sites.google.com/site/youngjunguh">Youngjung
                        Uh</a><h6>Yonsei University</h6></div>
                    <div class="people-name"><a target="_blank" href="https://www.etri.re.kr/eng/main/main.etri">Jang-Hee
                        Yoo</a><h6>ETRI</h6></div>
                    <div class="people-name"><a target="_blank" href="https://www.etri.re.kr/eng/main/main.etri">Kwangju
                        Kim</a><h6>ETRI</h6></div>
                    <div class="people-name"><a target="_blank" href="https://www.etri.re.kr/eng/main/main.etri">InSoo
                        Jang</a><h6>ETRI</h6></div>
                </div>
                <div class="col-xs-3">
                    <div class="people-name"><a target="_blank">Changsu Lee</a><h6>Youngnam University</h6></div>
                    <div class="people-name"><a target="_blank">SooYoung Kwak</a><h6>Hanbat University</h6></div>
                    <div class="people-name"><a target="_blank" href="https://ait.ethz.ch/people/spark/">Inkyu Park</a>
                        <h6>Inha University</h6></div>
                    <div class="people-name"><a target="_blank" href="https://natanielruiz.github.io/">SangHyun Park</a>
                        <h6>DIGIST</h6></div>
                </div>
            </div>
        </div>
    </div>

    <hr>
    <div class="row">
        <div class="col-xs-12">
            <h2>Acknowledgments</h2>
            <br>
            This workshop is proudly sponsored by <a href="https://cvpr.kmu.ac.kr/KAIFRI/">KMU(Keimyung University) Research Institute of AI fusion</a>.
        </div>
    </div>

    <hr>
    <div class="row">
        <div class="col-xs-12">
            <h2>Contact</h2>
            <br>
            For any related question, please contact Prof. Jong-Ha Lee (+82-10-8968-8769, segeberg@kmu.ac.kr)
        </div>
    </div>

</div>

</body>
</html>
